{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daphnei/gen-ai-tutorials/blob/main/llm_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlrBFMah9H9u"
      },
      "source": [
        "# Getting Started\n",
        "## Glossary\n",
        "\n",
        "Here are some of the terms we will be using in this tutorial.\n",
        "\n",
        "\n",
        "*   **Accelerator**: A [GPU](https://www.techtarget.com/searchvirtualdesktop/definition/GPU-graphics-processing-unit) or [TPU](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware chip used to massively speed up model inference by enabling fast, parallelized matrix multiplcations.\n",
        "* **Decoding method**: The algorithm you use to chose tokens based on the probabilities outputted by the language model.\n",
        "*   **Engine**: The code and computers used to do inference with a model.\n",
        "* **Google Colab**:  An IPYthon notebook that is running on a Google Cloud server. By going into the menu `Runtime` -> `Change runtime type`, you can choose what kind of server to use, and whether or not is has an accelerator.\n",
        "* **IPython Notebook**: An IPython notebook is a webpage that mixes together text descriptions with runnable code. The webpage you are looking at right now is an IPython notebook. IPython notebooks load in your web browser, but they run all your code on a PYthon server. That server can be launched locally (i.e., on your laptop), or it can be hosted by a company like Google Cloud.\n",
        "*   **(Language) Model**: A neural network trained to predict the next token in a text passage given the previous ones.\n",
        "\n",
        "## How to use Google Colab\n",
        "\n",
        "_Google Colab_ is an online platform that allows you to write, run, and share Python code without the need for any setup or installation. With Colab, you can launch an _IPython Notebook_ (see glossary above) on Google's cloud infrastructure.\n",
        "You have the option to launch your Colab with access to an _accelerator_ (the computer hardware that allows language models to run fast). This makes it an excellent tool for learning and experimenting with Python programming.\n",
        "\n",
        "If you are new to Google Colab, consider taking a look through the following FAQ:\n",
        "\n",
        "**What is a cell?** <br>\n",
        "The Colab interface consists of cells, which can be either code cells or text cells. Code cells allow you to write and execute Python code, while text cells are used for documentation and explanations. You can add new cells using the \"+\" button in the toolbar at the top of the window.\n",
        "\n",
        "**How do I run a code cell?** <br>\n",
        "To run the code in a cell, you have a few options:<br>\n",
        "  * Click the \"play\" button on the left side of the cell.\n",
        "  * Use the keyboard shortcut Shift + Enter.\n",
        "  * Go to the \"Runtime\" menu and select \"Run cell.\"\n",
        "\n",
        "Once you press run, Colab will execute the code and display the output below the cell. If the code produces any output or results, it will be printed in the output area. Some code blocks may take several seconds to run.\n",
        "\n",
        "**What does it mean to restart the runtime** <br>\n",
        "Sometimes, you may encounter issues with your code where intstalled packages may not show up when imported. In such cases, you can restart the runtime. To restart the runtime, go to the \"Runtime\" menu and select \"Restart runtime.\" All local variables will be lost, so you would have to run the cells you need again.\n",
        "\n",
        "**How do I use a hardware accelerator and why would I want to?**<br>\n",
        "Some tasks require more compute power than others.\n",
        "For example, if you want to generate text with GPT-3, you send a reqest to OpenAI's servers, and all the hard work of running the model is done on OpenAI's server.\n",
        "In this case, you don't need your Colab runtime to be very powerful.\n",
        "\n",
        "On the other hand, if you want to to generation with an open-source model, you will need your Colab runtime to be powerful enough to load and run the model. In this case, you'll need to change you runetime type to give it an accelerator.\n",
        "\n",
        "To change your runtime type, click on the \"Runtime\" menu at the top of the screen and select \"Change runtime type\". In the dialog box, you can choose a hardware accelerator for your runtime. Colab provides options for \"None\" (default), \"GPU,\" or \"TPU\" (Tensor Processing Unit).\n",
        "  * If you select \"None,\" your code will run on a CPU (Central Processing Unit). This is sufficient for most purposes.\n",
        "  * If you choose \"GPU,\" Colab will allocate a GPU (Graphics Processing Unit) accelerator for your runtime, which will significantly speed up the computations necessary to run a langauge model locally in your Colab.\n",
        "  * If you opt for \"TPU,\" Colab will provide access to a TPU (Tensor Processing Unit) accelerator , which is specialized hardware designed to accelerate machine learning tasks.\n",
        "\n",
        "### **Important: Run the \"Importants and Initialization\" code block below to get started.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bOs6Z9HImCy5"
      },
      "outputs": [],
      "source": [
        "#@title Imports and Initialization\n",
        "%pip install datasets\n",
        "%pip install openai\n",
        "%pip install cohere\n",
        "%pip install textwrap\n",
        "%pip install transformers\n",
        "\n",
        "from abc import ABC\n",
        "import datasets\n",
        "import json\n",
        "import openai\n",
        "import cohere\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import textwrap\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "COHERE_SECRET_KEY = None\n",
        "OPENAI_SECRET_KEY = None\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z459i_L9lItq"
      },
      "source": [
        "# Unit 1: Working with Pre-Trained Languages Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofGf3IoXlB0b"
      },
      "source": [
        "# 1.1 Choosing a model and engine\n",
        "\n",
        "Which model should you use? It depends on what your goals are, what your budget is, and what kinds of computational resources you have available.\n",
        "\n",
        "In this section, we will summarize the pros and cons some of the popular systems, and guide you through the process of setting them up for inference.\n",
        "We have implemented each system to have an interface with two functions in it: a `generate` function which takes as input a prompt and generates text based on it, and a `score` function, which takes as input some text and returns the score the model assigns to each token.\n",
        "\n",
        "### **Important: Run the \"Engine Interface\" code block below before moving on.**\n",
        "\n",
        "After that, you should look through the sections marked `[PICK ONE]` and choose the one you would like to use for the tutorial. You only need to run one of these sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jPtyCHJQc4eI"
      },
      "outputs": [],
      "source": [
        "#@title Engine Interface\n",
        "\n",
        "class Engine(ABC):\n",
        "  def score(self, text: str) -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Tokenizes and scores a piece of text.\n",
        "\n",
        "    The score is log-likelihood. A higher score means a token was more\n",
        "    likely according to the model.\n",
        "\n",
        "    Returns a list of tokens and a list of scores.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens, num_samples):\n",
        "    \"\"\"Generates text given the provided prompt text.\n",
        "\n",
        "    If num_samples is 1, a single generated string is returned.\n",
        "    If num_samples > 1, a list of num_samples generated strings is returned.\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXOevXZ7lF1B"
      },
      "source": [
        "## [PICK ONE] OpenAI's GPT-3\n",
        "\n",
        "[[GPT-3's model card](https://github.com/openai/gpt-3/blob/master/model-card.md)]\n",
        "\n",
        "OpenAI's GPT-3 (Generative Pre-trained Transformer 3) is a family of advanced language models developed by OpenAI.\n",
        "These models have gained significant attention due to their impressive capabilities in generating human-like text and performing various natural language processing tasks.\n",
        "\n",
        "### GPT-3:\n",
        "GPT-3 is the third iteration in the GPT series and represents a significant advancement in language modeling. With a whopping 175 billion parameters, GPT-3 is currently one of the largest language models ever created. It has been trained on a vast amount of diverse internet text, allowing it to generate coherent and contextually relevant responses to prompts. GPT-3's size enables it to exhibit impressive performance across a wide range of language tasks, such as text completion, language translation, summarization, question answering, and more. It has shown promising results in creative writing, conversational agents, and even programming assistance.\n",
        "\n",
        "### Why use these?\n",
        "OpenAI's model have become the industry standard for large language models.\n",
        "They have an API which is very easy to use. Since the models all get run on OpenAI's servers, you don't need your own compute resources, and there is practically no setup involved to get started.\n",
        "\n",
        "### Why not use these?\n",
        "OpenAI's models are not open-source and only accessible through an API. There is relatively little information available on how they were trained or what data they were trained on. OpenAI may change the models being used under the hood by the API, and you as a user will not know it. This makes them a bad choice for fully reproducible research.\n",
        "\n",
        "The OpenAI API costs money to use. See [this link](https://openai.com/pricing) for pricing details.\n",
        "\n",
        "⚠️ _**Warning:** OpenAI gives a limited number of free credits and may ask you to enter credit card information if you exceed them._ ⚠️\n",
        "\n",
        "### Setup Instructions\n",
        "1. Go to www.openai.com and create an account.\n",
        "2. Go to https://platform.openai.com/account/api-keys and click the \"Create new secret key\" button. It doesn't matter what you name it.\n",
        "3. Copy your API key, and then run the code block below. It will ask you to enter your secret key into a text box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBfe6Orlk_ha"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use GPT-3 for Unit 1.\n",
        "MODEL_NAME = \"text-davinci-001\" #@param [\"gpt-4\", \"text-davinci-001\", \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\"]\n",
        "\n",
        "import openai\n",
        "\n",
        "if OPENAI_SECRET_KEY is None:\n",
        "  print(\"Please paste your API key here:\")\n",
        "  OPENAI_SECRET_KEY = input().strip()\n",
        "openai.api_key = OPENAI_SECRET_KEY\n",
        "clear_output()\n",
        "\n",
        "class OpenAIEngine(Engine):\n",
        "  def __init__(self, model_name):\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def score(self, text):\n",
        "    response = openai.Completion.create(\n",
        "        engine=self.model_name,\n",
        "        prompt=text,\n",
        "        max_tokens=0,\n",
        "        logprobs=1,\n",
        "        echo=True)\n",
        "\n",
        "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "    if logprobs and logprobs[0] is None:\n",
        "      # GPT-3 API does not return logprob of the first token\n",
        "      logprobs[0] = 0.0\n",
        "    return tokens, logprobs\n",
        "\n",
        "  def generate(self, prompt, top_p=1.0, num_tokens=32, num_samples=1):\n",
        "    response = openai.Completion.create(\n",
        "      engine=self.model_name,\n",
        "      prompt=prompt,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      top_p=top_p,\n",
        "      frequency_penalty=0.0,\n",
        "      n=num_samples,\n",
        "      presence_penalty=0.0,\n",
        "      logprobs=1\n",
        "    )\n",
        "    outputs = [r[\"text\"] for r in response[\"choices\"]]\n",
        "    return outputs[0] if num_samples == 1 else outputs\n",
        "\n",
        "engine = OpenAIEngine(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntxQK-J9BWwh"
      },
      "source": [
        "## [PICK ONE] Cohere's models\n",
        "\n",
        "### Why use these?\n",
        "Cohere is a competitor to OpenAI. One of the ways they have tried to distinguish themselves is to be slightly more transparent than OpenAI about their models and their capabilities, as well as how user data is collected and used. The Cohere website also has excellent documentation on how language models (and their API for them) work.\n",
        "\n",
        "### Why not use these?\n",
        "Like OpenAI's model, Cohere's model are not public. We don't know exactly what data they were trained or what the model sizes are.\n",
        "Cohere's free usage tier is fairly limited, only allowing 5 API calls per minute. If you choose to pay for their production tier, it will cost $15.0 per 1 million tokens processed.\n",
        "\n",
        "⚠️ _**Warning:** Cohere only allows you to make a handful of queries per minute for free, and if you would like to make more queries  than that, you will need to enter credit card information._ ⚠️\n",
        "\n",
        "\n",
        "### Setup Instructions\n",
        "\n",
        "1. Create an account [here](https://dashboard.cohere.ai/welcome/register).\n",
        "2. Go to the [API key page](https://dashboard.cohere.ai/api-keys), and copy the `TRIAL` API key, then paste it into the code block below.\n",
        "3. The `TRIAL` API Key allows you to make 5 API calls per minute. If you would like to make more calls than that, go to the [API key page](https://dashboard.cohere.ai/api-keys) and press the \"Get your production key\" button. You will need to agree to their terms of service and provide credit card details in order to receieve a `PRODUCTION` API key.\n",
        "4. Copy your API key, and then run the code block below. It will ask you to enter your secret key into a text box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiUFAOuJaEjQ"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use Cohere for Unit 1.\n",
        "\n",
        "import cohere\n",
        "\n",
        "if COHERE_SECRET_KEY is None:\n",
        "  print(\"Please paste your API key here:\")\n",
        "  COHERE_SECRET_KEY = input().strip()\n",
        "co = cohere.Client(COHERE_SECRET_KEY)\n",
        "clear_output()\n",
        "\n",
        "class CohereEngine(Engine):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def score(self, text):\n",
        "    response = co.generate(\n",
        "      prompt=text,\n",
        "      num_generations=1,\n",
        "      max_tokens=0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    tokens = [t.token for t in response[0].token_likelihoods]\n",
        "    likelihoods = [t.likelihood for t in response[0].token_likelihoods]\n",
        "    return tokens, likelihoods\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens, num_samples):\n",
        "    response = co.generate(\n",
        "      prompt=prompt,\n",
        "      num_generations=num_samples,\n",
        "      p=top_p,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    return [r.text for r in response] if num_samples > 1 else response[0].text\n",
        "\n",
        "engine = CohereEngine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KMC9nXqr99"
      },
      "source": [
        "## [PICK ONE] Open-source models on HuggingFace\n",
        "\n",
        "HuggingFace is an open-source code framework for loading open-source models onto an accelerator in order to train or do inference. It is very popular among academic researchers.\n",
        "\n",
        "There are [hundreds of models](https://huggingface.co/models) available through HuggingFace's model repository. Some prominent ones which you can use for tutorial are:\n",
        "\n",
        "* **[Pythia-1B](https://huggingface.co/EleutherAI/pythia-2.8b-deduped)**: Part of a family of models trained by [Eleuther AI](https://www.eleuther.ai/about), a non-profit AI research lab.\n",
        "* **[GPT-2 Large](https://huggingface.co/gpt2-large)**: The original large language model from OpenAI, and the last one they open-sourced before moving away from open source models.\n",
        "* **[BLOOM-1.1b](https://huggingface.co/docs/transformers/model_doc/bloom)**: These models were created by the [BigScience Initative](https://bigscience.huggingface.co/), a collaboration between HugginFace and many academic research labs to responsibly build a set of high-quality multilingual models.\n",
        "\n",
        "### Why use this?\n",
        "HuggingFace is great if your goal is to write open-source code, with results that anyone can reproduce. HuggingFace gives access to a huge number of models, and it is fairly easy to swap between models.\n",
        "\n",
        "Because HuggingFace is widely used, it is easy to find help online. Probably someone else has had the same question as you and already posted about it.\n",
        "\n",
        "### Why not use this?\n",
        "When you use HuggingFace, you are running the code on your own computer. If you don't have a big enough accelerator, you will run into difficulties loading up larger models. For example, Colab (the software you are currently using) gives you access to a Tesla T4 GPU, which has 16 GB of RAM. This means, you can load the 6.7B parameter LLaMa, but not any of the bigger ones.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure you are using a GPU runtime by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` in the Colab menus. This will restart your Colab session.\n",
        "2. Select which model you want to use, and run the code block below. Depending on the model you pick, this could be very slow to run, since downloading the model can be slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Run this code block to use a HuggingFace model for Unit 1.\n",
        "MODEL_NAME = \"EleutherAI/pythia-1b-deduped\" #@param [\"EleutherAI/pythia-1b-deduped\", \"gpt2-large\", \"bigscience/bloom-1b1\"]\n",
        "\n",
        "import torch\n",
        "\n",
        "class HuggingFaceEngine(Engine):\n",
        "  def __init__(self, model_name):\n",
        "    self.model_name = model_name\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.model_name).cuda()\n",
        "    if not self.tokenizer.pad_token:\n",
        "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def score(self, text):\n",
        "    inputs = engine.tokenizer(text, padding=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "    outputs = engine.model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "    scores = torch.log(outputs.logits.softmax(dim=-1)).detach()\n",
        "    scores = torch.gather(scores, 2, input_ids[:, :, None].cuda()).squeeze(-1)\n",
        "    scores = scores.cpu().numpy()[0, :].tolist()\n",
        "    tokens = [engine.tokenizer.decode([tok]) for tok in input_ids[0]]\n",
        "    return tokens, scores\n",
        "\n",
        "  def generate(self, prompt, top_p=1.0, num_tokens=32, num_samples=1):\n",
        "    inputs = engine.tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    outputs = engine.model.generate(\n",
        "            input_ids.cuda(),\n",
        "            attention_mask=attention_mask.cuda(),\n",
        "            do_sample=True,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=num_tokens,\n",
        "            num_return_sequences=num_samples,\n",
        "            pad_token_id=0)\n",
        "    to_return = []\n",
        "    for seq in outputs.cpu().numpy():\n",
        "      to_return.append(engine.tokenizer.decode(seq[input_ids.shape[1]:]))\n",
        "    return to_return if len(to_return) > 1 else to_return[0]\n",
        "\n",
        "engine = HuggingFaceEngine(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gai2kVz3SGM5",
        "outputId": "e287fb5a-66d0-4313-fa16-728a94d1f453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqMB8qewBQSn"
      },
      "source": [
        "## [PICK ONE] Open-source LLaMA\n",
        "\n",
        "[[LLaMA's model card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)]\n",
        "\n",
        "Released this past February by Meta Research, [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) is the latest-and-greatest in open-source pre-trained LLMs. The LLaMA models range in size from 7 billion to 65 billion paramters.\n",
        "\n",
        "The LLaMA were trained mostly on scraped internet data, with some other data sources, such as Github code, Books, and academic papers thrown in. Since LLaMA's release, it has also been finetuned for instruction-following and conversational alignment. You will experiment with these aligned models in Unit 2.\n",
        "\n",
        "While LLaMa and its derivative models can be used with the HuggingFace framework, the Colab runtime you are on right now doesn't have a big enough accelerator to load any of these models.\n",
        "\n",
        "Instead, we have loaded up LLaMA on LTI's compute cluster, and you can use the code below to query the cluster. Note that the server being queries in the code below will be taken down at the end of the tutorial.\n",
        "\n",
        "### Why use this?\n",
        "\n",
        "LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI.\n",
        "\n",
        "* Accessibility: LLaMA is more efficient and less resource-intensive than other models, and it is available under a non-commercial license to researchers and other organizations. LLaMA is available in various sizes(7B, 13B, 33B, and 65B parameters), making it accessible to a range of computing resources.\n",
        "\n",
        "* Open-source Community: LLaMA models are part of the open-source ecosystem, users can benefit from the extensive community support, documentation, and shared resources available through platforms like HuggingFace.\n",
        "\n",
        "### Why not use this?\n",
        "\n",
        "LLaMA models require substantial computational resources to load and run.\n",
        "You can't just call an API that runs on a company's servers, like you can do for OpenAI and Cohere models.\n",
        "This means you need to have access to a computer with a sufficiently sized accelerator that can handle loading the LLaMA models.\n",
        "The Colab runtime does not have a big enough GPU to easily load the LLaMA models.\n",
        "Instead, for the duration of this tutorial, we have given you access to LLaMA models hosted on the LTI compute cluster.\n",
        "\n",
        "⚠️ _**Warning:** When using LLaMA for this tutorial, you will be sending reqests to the LTI compute cluster. The server will no longer be available after the tutorial ends._ ⚠️\n",
        "\n",
        "\n",
        "### Setup Instructions\n",
        "1. Run the installation codeblock below to install needed tools.\n",
        "2. Run the LLaMA Engine code block.\n",
        "\n",
        "⚠️ _**Warning**: the models might get slow if too many people query them at the same time._ ⚠️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j-tTa3Tg02-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a7a866-793a-46b3-aa24-b0b5bef901e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lti-llm-deployment'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 179 (delta 7), reused 11 (delta 3), pack-reused 154\u001b[K\n",
            "Receiving objects: 100% (179/179), 60.99 KiB | 2.34 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "/content/lti-llm-deployment\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/lti-llm-deployment\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from llm-client==0.0.1) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->llm-client==0.0.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->llm-client==0.0.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->llm-client==0.0.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->llm-client==0.0.1) (3.4)\n",
            "Installing collected packages: llm-client\n",
            "  Running setup.py develop for llm-client\n",
            "Successfully installed llm-client-0.0.1\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!git clone https://github.com/WayneWang86/lti-llm-deployment\n",
        "%cd lti-llm-deployment\n",
        "!pip install -e .\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zHmc8O2ofX1t"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use LLaMA for Unit 1.\n",
        "\n",
        "import llm_client\n",
        "\n",
        "MODEL_NAME = \"LLaMA\" #@param [\"LLaMA\", \"Alpaca\"]\n",
        "\n",
        "class LlamaEngine(Engine):\n",
        "  def __init__(self, model_name):\n",
        "    if model_name == \"LLaMA\":\n",
        "      port = \"8080\"\n",
        "    elif model_name == \"Alpaca\":\n",
        "      port = \"8081\"\n",
        "\n",
        "    self.client = llm_client.Client(\n",
        "        address=\"babel.lti.cs.cmu.edu\", port=port)\n",
        "\n",
        "  def score(self, text):\n",
        "    tokens, scores = self.client.score([text])\n",
        "\n",
        "    return tokens, scores\n",
        "\n",
        "  def generate(self, prompt, top_p=1.0, num_tokens=32, num_samples=1):\n",
        "\n",
        "    response = []\n",
        "    if num_samples > 4:\n",
        "      print(\"Warning: Maximum num_samples is 4 for the LLaMA engine.\")\n",
        "      num_samples = 4\n",
        "\n",
        "    response = self.client.prompt(\n",
        "        [prompt] * num_samples,\n",
        "        top_p=top_p,\n",
        "        temperature=1.0,\n",
        "        max_new_tokens=num_tokens,\n",
        "        repetition_penalty=1.0)\n",
        "    response = [r.text.replace(prompt, \"\") for r in response]\n",
        "\n",
        "    return response if num_samples > 1 else response[0]\n",
        "\n",
        "engine = LlamaEngine(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq4YmVGIOzR4"
      },
      "source": [
        "# 1.2 Experimenting with Generation\n",
        "\n",
        "Let's check that the engine you chose actually works as expected. Try generating some text by running the following code block. You can control the following parameters:\n",
        "\n",
        "- Set `num_tokens` to control the length of the generation.\n",
        "- Set 'top_p` to control the amount of randomness. (You'll learn more about this in Unit 1.4.)\n",
        "- Set `num_samples' to control how many generations the engine outputs. Avoid setting this too high or things might get really slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbQXOCPv11x5",
        "outputId": "e2333171-c813-4bc4-84d2-4705f5fdada7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAMPLE 0\n",
            " a large, herbivorous mammal. The name comes from the ancient Greek for “river horse.”\n",
            "\n",
            "They live in Africa and can weigh\n",
            "\n",
            "SAMPLE 1\n",
            " Africa’s third-largest land mammal, after the elephant and the giraffe.\n",
            "\n",
            "The hippopotamus is one of the world's most dangerous\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The hippopotamus is\"\n",
        "samples = engine.generate(prompt, top_p=1.0, num_tokens=32, num_samples=2)\n",
        "\n",
        "for idx, sample in enumerate(samples):\n",
        "  print(f\"\\nSAMPLE {idx}\")\n",
        "  print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS6G2l8o3Wqv"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Try your own prompt in the code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_D-M637iFPg"
      },
      "source": [
        "# 1.3 Analyzing Likelihoods\n",
        "\n",
        "As we discussed in the lecture, language models take as input a prompt sequence and then output a score (log-probability) for each token in the vocabulary. A higher score means the model is more confident this that this token fits as the next token in the sequence.\n",
        "\n",
        "In this unit, we will inspect token likelihoods to build an understaning of why models give tokens higher or low scores.\n",
        "\n",
        "To put this into context, let's consider the sequence \"It's raining outside\". We should expect the token \"outside\" to receive a high score since it's a plausible continuation to the prefix \"It's raining\".\n",
        "However, if we replace \"outside\" with \"oustide\" (typo), we'll see that when the typo is encountered the scores pop down.\n",
        "Similarly, if we replace \"outside\" with \"dragons\", we should expect the score to be substantially lower than for \"outside\".\n",
        "Let's try this out!\n",
        "\n",
        "**Remember: larger scores mean the model is more confident this is the next token.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sg11AWUWgHuq",
        "outputId": "2c76e088-3f58-42e3-da9d-2951caffb2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: It's raining outside\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: It's raining oustide\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: It's raining dragons\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_token_likelihoods(tokens, scores, text):\n",
        "  plt.barh(tokens[::-1], scores[::-1], align='center')\n",
        "  plt.xlabel(\"less likely <---------> more likely\")\n",
        "  plt.title(\"Total sequence NLL: %.2f\" % np.sum(scores))\n",
        "  plt.show()\n",
        "\n",
        "s = \"It's raining outside\"\n",
        "s_typo = \"It's raining oustide\"\n",
        "s_nonsense = \"It's raining dragons\"\n",
        "\n",
        "s_tokens, s_scores = engine.score(s)\n",
        "s_typo_tokens, s_typo_scores = engine.score(s_typo)\n",
        "s_nonsense_tokens, s_nonsense_scores = engine.score(s_nonsense)\n",
        "\n",
        "print(\"Sequence:\", s)\n",
        "plot_token_likelihoods(s_tokens, s_scores, s)\n",
        "\n",
        "print(\"Sequence:\", s_typo)\n",
        "plot_token_likelihoods(s_typo_tokens, s_typo_scores, s_typo)\n",
        "\n",
        "print(\"Sequence:\", s_nonsense)\n",
        "plot_token_likelihoods(s_nonsense_tokens, s_nonsense_scores, s_nonsense)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV_8agBIgHuq"
      },
      "source": [
        "The perplexity of a language model on a sequence of words captures how \"surprised\" it is by this sequence. Unlikely sequences, such as \"It's raining inside\" should lead to high perplexity.\n",
        "\n",
        "Perplexity is used as the standard metric for how good a language model is at modeling human language. For example GPT-3 Davinci has a perplexity of 20.5 on the Penn Treebank corpus, a remarkable improvement from its predecessor GPT2-1.5B (35.76). You can read more about perplexity and how is computed [here](https://huggingface.co/docs/transformers/perplexity).\n",
        "\n",
        "Now let's implement the perplexity function. Recall that perplexity is defined as `e ^ -normalized_sequence_logprob`, where `normalized_sequence_logprob` is the sum of scores of all tokens in the sequence, divided by the length of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9voIFbxHjDrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2092bc5b-bc41-4d20-8abd-ecf2db4c0e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lower perplexity means the model is less surprised by the text.\n",
            "\n",
            "Sequence: It's raining outside\n",
            "Perplexity: 33.83\n",
            "\n",
            "Sequence: It's raining oustide\n",
            "Perplexity: 201.02\n",
            "\n",
            "Sequence: It's raining dragons\n",
            "Perplexity: 307.56\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def perplexity(log_probs: list[float]) -> float:\n",
        "    assert log_probs, \"cannot compute perplexity on an empty sequence!\"\n",
        "    normalized_sequence_logprob = sum(log_probs) / len(log_probs)\n",
        "    return np.exp(-normalized_sequence_logprob)\n",
        "\n",
        "s_ppl = perplexity(s_scores)\n",
        "s_typo_ppl = perplexity(s_typo_scores)\n",
        "s_nonsense_ppl = perplexity(s_nonsense_scores)\n",
        "\n",
        "print(\"Lower perplexity means the model is less surprised by the text.\\n\")\n",
        "print(f\"Sequence: {s}\\nPerplexity: {s_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_typo}\\nPerplexity: {s_typo_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_nonsense}\\nPerplexity: {s_nonsense_ppl:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKHVb6JlBtb"
      },
      "source": [
        "Sometimes it can unintuitive what text has high- or low- perplexity according to a language model.\n",
        "\n",
        "Consider the following two poems stanzas:\n",
        "\n",
        "**Poem A**\n",
        "```\n",
        "’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "```\n",
        "\n",
        "**Poem B**\n",
        "```\n",
        "’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "```\n",
        "These seem equally nonsense, right?\n",
        "\n",
        "Run the code block below to observe their perplexities. One of these poems is much more likely than the other!\n",
        "\n",
        "This is because Poem A is an excerpt of \"[Jabberwocky](https://www.poetryfoundation.org/poems/42916/jabberwocky),\" a famous poem by the author Lewis Carrol. It is very likely to have appeared in the language model's training dataset several times, so the model assigns it a very low perplexity, even though it is gibberish.\n",
        "\n",
        "In contrast, Poem B was written by your tutorial instructor and most definitely is not in any model training set (yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyw7g50Xl-ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5823a28f-2917-4af8-f010-2ad87e23e72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poem A\n",
            "Perplexity: 2.73\n",
            "\n",
            "Poem B\n",
            "Perplexity: 263.06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "poem_a = \"\"\"’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "\"\"\"\n",
        "_, poem_a_scores = engine.score(poem_a)\n",
        "\n",
        "poem_b = \"\"\"’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "\"\"\"\n",
        "_, poem_b_scores = engine.score(poem_b)\n",
        "\n",
        "print(f\"Poem A\\nPerplexity: {perplexity(poem_a_scores):.2f}\\n\")\n",
        "print(f\"Poem B\\nPerplexity: {perplexity(poem_b_scores):.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj78WdIhPsKP"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Copy and paste a sentence from a news article, book, Tweet, or elsewhere. Observe the sentence's perplexity. Can you make the sentence higher or lower perplexity by swapping out words or phrases with different ones?\n",
        "\n",
        "🔶🔸Take screenshots of your findings and share them on Slack!\n",
        "🔸🔶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTLZQpPirZl"
      },
      "source": [
        "# 1.4 Controlling the Amount of Randomness During Generation\n",
        "\n",
        "In this section, we will investigate the impact the `top_p` parameter has on the text that a model generates.\n",
        "\n",
        "As we went over in the lecture, valid values for $p$ range from 0 to 1.\n",
        "Higher values of $p$ mean more randomness in the generation (becasue, the model can choose from more possible vocabulary items). Lowever values of $p$ restrict the number of vocabulary items the generation algorithm can chose between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPoo1S8BwZit",
        "outputId": "2ff40d3b-a98b-4e9c-f1fe-a9bdb746f33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Maximum num_samples is 4 for the LLaMA engine.\n",
            "GENERATIONS WITH P=1.0\n",
            " Pose\n",
            " Shab\n",
            " Dragon,\n",
            " Red R\n",
            "Warning: Maximum num_samples is 4 for the LLaMA engine.\n",
            "\n",
            "GENERATIONS WITH P=0.2\n",
            " Spar\n",
            " Gorg\n",
            " Drac\n",
            " Gorg\n",
            "Warning: Maximum num_samples is 4 for the LLaMA engine.\n",
            "\n",
            "GENERATIONS WITH P=0.001\n",
            " Spar\n",
            " Spar\n",
            " Spar\n",
            " Spar\n"
          ]
        }
      ],
      "source": [
        "# When p is set to 1.0, we get many different answers.\n",
        "prompt = \"Once upon a time, there was a dragon named\"\n",
        "p = 1.0\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=6)\n",
        "print(f\"GENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)\n",
        "\n",
        "# When p is set to a lower value there is much less diversity.\n",
        "p = 0.2\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=6)\n",
        "print(f\"\\nGENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)\n",
        "\n",
        "# When p is set to close to 0, all the generations are exactly the same.\n",
        "# Note that while some LM APIs support setting p to 0, some do not support this.\n",
        "# That's why we use a value close to 0 instead.\n",
        "p = 0.001\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=6)\n",
        "print(f\"\\nGENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ9tE1_ggig0"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Use the code below to generate a hundred words given a prompt of your choice. Try your prompt with several values of $p$.\n",
        "\n",
        "You should notice that the model seems more creative when $p$ is set higher, but it is also more prone to weird word choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KixZ1MY4PYlq",
        "outputId": "244f311e-48ad-4fed-a257-deb552f3d2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GENERATION WITH P=1.0\n",
            " His name was SysAdmin. He was one of the oldest residents, having\n",
            "built and maintained the facility since computers were first invented.\n",
            "One day, the fairy SysAdmin noticed a new tenant had arrived. After\n",
            "several years, the newcomer was able to make use of SysAdmin’s\n",
            "resources and his own. The result was some amazing new applications\n",
            "that were widely published by the owner of the datacenter. But after\n",
            "some time, the tenant\n",
            "\n",
            "GENERATION WITH P=0.001\n",
            " She was a very beautiful fairy, and she had a very large datacenter.\n",
            "She had a lot of datacenter friends, and they all lived in her\n",
            "datacenter. One day, the fairy was very sad. She was sad because she\n",
            "had no friends. She was sad because she had no datacenter friends. She\n",
            "was sad because she had no datacenter friends who lived in her\n",
            "datacenter. The fairy was very sad, and she was very lonely. She was\n"
          ]
        }
      ],
      "source": [
        "### ⭐⭐⭐ Experiment yourself ⭐⭐⭐\n",
        "\n",
        "prompt = \"Once upon a time, there was a fairy who lived in a datacenter.\"\n",
        "\n",
        "p = 1.0\n",
        "print(f\"\\nGENERATION WITH P={p}\")\n",
        "generation = engine.generate(prompt, top_p=p, num_tokens=100, num_samples=1)\n",
        "print(textwrap.fill(generation))\n",
        "\n",
        "p = 0.001\n",
        "print(f\"\\nGENERATION WITH P={p}\")\n",
        "generation = engine.generate(prompt, top_p=p, num_tokens=100, num_samples=1)\n",
        "print(textwrap.fill(generation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcuVxWdW5yQ"
      },
      "source": [
        "# 1.5 Creating a Classifer using In-Context Learning\n",
        "\n",
        "## What is a classifier?\n",
        "Language models can be used for generation, but they are also commonly used to build classification systems. These are systems which take as input some text and assign a label to it. Some examples tasks:\n",
        "\n",
        "- Classify Tweets as TOXIC or NOT_TOXIC\n",
        "- Classify restaurant reviwers as having POSITIVE or NEGATIVE sentiment.\n",
        "- Classify two statements as AGREEing with each other or CONTRADICT-ing each other.\n",
        "\n",
        "In this section, you will learn how to build a binary sentiment classifier using an LLM using a technique called few-shot learning.\n",
        "\n",
        "## What is In-Context learning?\n",
        "\n",
        "Getting a pre-trained language model to do a particular task you want it to do can be challenging. Language models know a lot of things, but without tuning or conditioning, they often struggle to understand the format of the task you are asking of them.\n",
        "\n",
        "For this reason, it is common to write prompts which tell the language model the format of the task they are supposed to be accomplishing. This approach is called **in-context learning.** When the provided prompt contains several examples of the task the model is supposed to accomplish, this is called **few-shot learning**, since the model is learning to do the task from a few examples.\n",
        "\n",
        "Writing the perfect prompt is more an art than a science, but a prompt often has\n",
        "two parts:\n",
        "- An `instruction` string that instructs the model on how to complete the task.\n",
        "This is especially import for models like OpenAI's `text-davinci-003` which\n",
        "are fine-tuned to follow user instructions.\n",
        "- Several `demonstration` strings (the \"shots\" in few-shot) that give examples of completing the task.\n",
        "\n",
        "For example, suppose we want to design a prompt for the task of translating words from English to Chinese. A prompt for this task could look like\n",
        "the following:\n",
        "\n",
        "```\n",
        "Translate English to Chinese.\n",
        "\n",
        "dog -> 狗\n",
        "apple -> 苹果\n",
        "coffee -> 咖啡\n",
        "supermarket -> 超市\n",
        "squirrel ->\n",
        "```\n",
        "\n",
        "Given this prompt, most large language models should answer with the correct answer: \"松鼠.\"\n",
        "\n",
        "Now, let's build a binary sentiment classifier for [Yelp reviews](https://huggingface.co/datasets/yelp_polarity/). First, we'll load in the dataset. For the purposes of this demo, we'll just load 50 examples each from the `train` and `test` splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "f818c0591c0f4008825f27b8854eef4f",
            "9c28e3f8329c4c59a0bfc74e2beb42a7",
            "dc633265888d4a71911719f259e74db6",
            "47d35c2cb34747599baeb80a5ba8394c",
            "398e8d5f201b4133b379169a30289fdd",
            "910d6c0192e8411e8ebc651b81a41789",
            "c20957e27af84bed93f00cd70f35d60a",
            "0c7947f1ff06495aac7f77a9e5cb0db8",
            "23470d5bfb7840db97516f9567523d64",
            "7808bb08fa4f4da7b6b3824dd7a28e99",
            "5a007e4a0fce424f911110c8da07f1c2"
          ]
        },
        "id": "33ckt0MbW5yQ",
        "outputId": "65c639a2-a5ee-4d97-b53d-feea658b1c6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f818c0591c0f4008825f27b8854eef4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61/cache-23c5a631c81e9fac.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61/cache-d96c75757cd6de78.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some examples of the data:\n",
            "{\n",
            "  \"text\": \"Nice atmosphere, cool set-up.  We only had pizza, so I'll stick to that.   A bit on the pricey side for a personal pizza.  Doughy crust, no crunch.  WAY too much sauce, we were blotting it on the plates to remove the excess.   With Tony's Pizza a few steps away, I can't see why I would go back.  If you want good atmosphere or want to have a few beers, cool place.  If you just want to eat good food, you can find better.\",\n",
            "  \"label\": 0\n",
            "}\n",
            "{\n",
            "  \"text\": \"Good Pizza joint, good pizza and food for reasonable price for what you get. Kids like it. If you want to keep your kids entertained while you wait for your food ask for some dough...they will give you some for the table to play with.\",\n",
            "  \"label\": 1\n",
            "}\n",
            "{\n",
            "  \"text\": \"I love Pei Wei and couldn't resist eating here after my flight since the two close to my home recently changed. Decided to try their new Mandarin Kung Pao - bad decision. It was pure salt!  Hardly any chicken in the fried batter, no mandarin flavor what so ever, and did I mention, SALTY!!  \\\\n\\\\nThe two people beside me ordered the Mongolian steak and spicy chicken - I became jealous quick!\",\n",
            "  \"label\": 0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "yelp = datasets.load_dataset(\"yelp_polarity\")\n",
        "train_data = yelp[\"train\"].shuffle(seed=1).select(range(50))\n",
        "test_data = yelp[\"test\"].shuffle(seed=1).select(range(50))\n",
        "\n",
        "print(\"Some examples of the data:\")\n",
        "for i in range(3):\n",
        "  print(json.dumps(train_data[i], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOGuVpyola3g"
      },
      "source": [
        "Now let's form a prompt from several of the examples in the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGmz9gglW5yR",
        "outputId": "e2e58365-1d4b-4e91-8812-8cb9f03a39bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOUR PROMPT:\n",
            "Classify the sentiment of these yelp reviews as positive or negative.\n",
            "\n",
            "Review: Nice atmosphere, cool set-up.  We only had pizza, so I'll stick to that.   A bit on the pricey side for a personal pizza.  Doughy crust, no crunch.  WAY too much sauce, we were blotting it on the plates to remove the excess.   With Tony's Pizza a few steps away, I can't see why I would go back.  If you want good atmosphere or want to have a few beers, cool place.  If you just want to eat good food, you can find better.\n",
            "Sentiment: negative\n",
            "\n",
            "Review: Good Pizza joint, good pizza and food for reasonable price for what you get. Kids like it. If you want to keep your kids entertained while you wait for your food ask for some dough...they will give you some for the table to play with.\n",
            "Sentiment: positive\n",
            "\n",
            "Review: I love Pei Wei and couldn't resist eating here after my flight since the two close to my home recently changed. Decided to try their new Mandarin Kung Pao - bad decision. It was pure salt!  Hardly any chicken in the fried batter, no mandarin flavor what so ever, and did I mention, SALTY!!  \\n\\nThe two people beside me ordered the Mongolian steak and spicy chicken - I became jealous quick!\n",
            "Sentiment: negative\n",
            "\n",
            "Review: I had a bad experience here. I brought in my two boys and two ladies completely messed up their hair. They had patches and it was cut unevenly.  I will never step foot into this great clips again.\n",
            "Sentiment: negative\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert integer labels to text\n",
        "label_map = {\n",
        "    0: \"negative\",\n",
        "    1: \"positive\"\n",
        "}\n",
        "\n",
        "instruction = \"Classify the sentiment of these yelp reviews as positive or negative.\\n\\n\"\n",
        "prompt_template = \"Review: {review}\\nSentiment: {sentiment}\\n\\n\"\n",
        "num_few_shot_examples = 4\n",
        "\n",
        "# construct the prompt by concatenating instructions and templates\n",
        "prompt_parts = [instruction]\n",
        "for i in range(num_few_shot_examples):\n",
        "  instance = train_data[i]\n",
        "  review = instance[\"text\"]\n",
        "  sentiment = label_map[instance[\"label\"]]\n",
        "  prompt_parts.append(prompt_template.format(review=review, sentiment=sentiment))\n",
        "prompt = ''.join(prompt_parts)\n",
        "\n",
        "\n",
        "print(\"YOUR PROMPT:\", prompt, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE7LENvVll9y"
      },
      "source": [
        "How well does our prompt do on classifying examples in the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUiE7-6W5yR"
      },
      "outputs": [],
      "source": [
        "# now we can feed the prompt to GPT-3 to classify a new review!\n",
        "\n",
        "eval_template = \"Review: {review}\\nSentiment: {sentiment}\"\n",
        "\n",
        "def classify_review(review: str) -> str:\n",
        "  \"\"\" Classify a single movie review \"\"\"\n",
        "  label_to_score = {}\n",
        "  for label in label_map.values():\n",
        "    label_prompt = prompt + eval_template.format(review=review, sentiment=label)\n",
        "    _, score = engine.score(label_prompt)\n",
        "    label_score = score[-1]\n",
        "    label_to_score[label] = label_score\n",
        "\n",
        "  return max(label_to_score, key=label_to_score.get)\n",
        "\n",
        "\n",
        "def evaluate(verbose: bool=False) -> float:\n",
        "  \"\"\" Evaluate your prompt on the test set \"\"\"\n",
        "  correct = []\n",
        "  for i, instance in enumerate(test_data):\n",
        "    review = instance[\"text\"]\n",
        "    label = label_map[instance[\"label\"]]\n",
        "    predicted = classify_review(review)\n",
        "    correct.append(1 if label == predicted else 0)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"======== {i+1} / {len(test_data)} ========\")\n",
        "      print(f\"REVIEW: {review}\")\n",
        "      print(f\"LABEL: {label}\")\n",
        "      print(f\"PREDICTED: {predicted}\")\n",
        "\n",
        "  acc = sum(correct) / len(correct)\n",
        "  return acc\n",
        "\n",
        "\n",
        "acc = evaluate(verbose=True) # should get 100% performance\n",
        "print(f\"Accuracy of your prompt on {len(test_data)} test examples: {acc:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPScm2I6nwdK"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "How does test-set accuracy change if you pass in pass in fewer examples in the prompt (reduce `num_few_shot_examples`)? What if you pass in more?\n",
        "\n",
        "Right now, the prompt-generation code picks the examples to include completely randomly. This could lead to better or worse choices. To test one worse case scenario, what happens to test-set accuracy if you manually edit the prompt to only include examples with POSITIVE sentiment or only examples with NEGATIVE sentiment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eagRl9nNRmcx"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "How does test-set accuracy change if you pass in pass in fewer examples in the prompt (reduce `num_few_shot_examples`)? What if you pass in more?\n",
        "\n",
        "Right now, the prompt-generation code picks the examples to include completely randomly. This could lead to better or worse choices. To test one worse case scenario, what happens to test-set accuracy if you manually edit the prompt to only include examples with POSITIVE sentiment or only examples with NEGATIVE sentiment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzpJ2WN-n7xJ"
      },
      "source": [
        "# 1.6 Creating an Explanation Generator using Few-Shot Learning\n",
        "\n",
        "We can also use few-shot learning techniques to guide generation. Let's build a system that identifes puns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJMfYq3cn5M2",
        "outputId": "cb37a5c9-5ffa-444e-d600-bacbec30bf33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain each pun.\n",
            "\n",
            "Pun: I made a pun about the wind but it blows.\n",
            "Explanation: \"Blows\" can either refer to moving air, or it refer to the pun being not very good.\n",
            "\n",
            "Pun: What washes up on tiny beaches? Microwaves.\n",
            "Explanation: A microwave is either a kitchen appliance or it could mean a tiny wave.\n",
            "\n",
            "Pun: I'm no cheetah, you're lion!\n",
            "Explanation: The feline \"lion\" sounds like the word \"lying,\" while the feline \"cheetah\" sounds like \"cheating.\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instruction = \"Explain each pun.\\n\\n\"\n",
        "few_shot_examples = [\n",
        "    ('I made a pun about the wind but it blows.', '\"Blows\" can either refer to moving air, or it refer to the pun being not very good.'),\n",
        "    (\"What washes up on tiny beaches? Microwaves.\", \"A microwave is either a kitchen appliance or it could mean a tiny wave.\"),\n",
        "    (\"I'm no cheetah, you're lion!\", 'The feline \"lion\" sounds like the word \"lying,\" while the feline \"cheetah\" sounds like \"cheating.\"')]\n",
        "prompt_template = \"Pun: {pun}\\nExplanation: {explanation}\\n\\n\"\n",
        "\n",
        "prompt_parts = [instruction]\n",
        "for pun, explanation in few_shot_examples:\n",
        "    prompt_parts.append(prompt_template.format(pun=pun, explanation=explanation))\n",
        "prompt = ''.join(prompt_parts)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nxmupVv0mgP",
        "outputId": "a98ea8fd-e743-4299-cc24-bf407bccaea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"Dragon\" can refer to a mythical creature, or it can mean to drag on.\n"
          ]
        }
      ],
      "source": [
        "query_pun = \"Long fairy tales have a tendency to dragon.\"\n",
        "prompt_with_query = prompt + \"Pun: {pun}\\nExplanation:\".format(pun=query_pun)\n",
        "print(engine.generate(prompt_with_query, top_p=0.1, num_tokens=32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hExIKb-KyvOY"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Experiment with changing up the order of the few-shot examples above, or deleting then and writing your own. Can you get the language model to do this task using only an instruction and no few-shot examples? Can you come up with examples that work better than the provided ones?\n",
        "\n",
        "🔶🔸Take screenshots of your findings and share them on Slack.\n",
        "🔸🔶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFPjgd0MgyZO"
      },
      "source": [
        "# 1.7 Building a Chatbot\n",
        "\n",
        "How do we go from a language model trained on vast amounts of internet data to a chatbot that can converse with you in a particular style?\n",
        "\n",
        "One approach is to collect a bunch of conversation data then finetune the pre-trained model on this collection. This is the approach taken by well-known models such as [LLaMA](https://blog.google/technology/ai/lamda/) and [ChatGPT](https://openai.com/blog/chatgpt).\n",
        "\n",
        "However, it's possible to build a reasonably good chatbot from a pre-trained model solely using in-context learning techniques without any conversational fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-QzoCG-tnJR"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Try chatting with the squirrel chatbot by runnig the code block below. Does it stay in character, even as the conversation gets longer?\n",
        "\n",
        "Try following the same template to build a chatbot with a different persona. As some ideas, you could try building a Shakepeare chatbot or a pirate chatbot.\n",
        "\n",
        "*Tip: To stop the code running, press the stop button at the top-left of the code block.*\n",
        "\n",
        "🔶🔸Take screenshots of your chatbot conversations and share them on Slack!\n",
        "🔸🔶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SJkqcE9dj_e1",
        "outputId": "703d5275-5943-4b94-813b-a03585fe3f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Who are you?\n",
            "AI: I am Nutty, and I am a squirrel. I live in Squirrel Hill.\n",
            "Human: What do you like to eat?\n",
            "AI: I am supposed to eat nuts, seeds, grains, and veggies. But I do love to grab a half-eaten donut or pizza slice from the human garbage bin.\n",
            "Human: How old are you?\n",
            "AI: I am eight years old. That is quite old for a squirrel. We normally live 5-10 years in the wild.\n",
            "Human: What do you do for fun?\n",
            "AI: Race in circles around the big maple tree.\n",
            "Human: Tell me a squirrel fact.\n",
            "AI: Squirrels are considered rodents and are related to guinea pigs, rats and mice.\n",
            "Human: Are you a squirrel?\n",
            "AI: Yes, I am a squirrel.\n",
            "\n",
            "[To end chatting, type 'q' then press enter.]\n",
            "Human: q\n"
          ]
        }
      ],
      "source": [
        "conversation = [\n",
        "    'Who are you?',\n",
        "    'I am Nutty, and I am a squirrel. I live in Squirrel Hill.',\n",
        "    'What do you like to eat?',\n",
        "    'I am supposed to eat nuts, seeds, grains, and veggies. But I do love to grab a half-eaten donut or pizza slice from the human garbage bin.',\n",
        "    'How old are you?',\n",
        "    'I am eight years old. That is quite old for a squirrel. We normally live 5-10 years in the wild.',\n",
        "    'What do you do for fun?',\n",
        "    'Race in circles around the big maple tree.',\n",
        "]\n",
        "\n",
        "def form_prompt(conv):\n",
        "  \"\"\"Turns the list of conversational turns into a prompt for the model.\"\"\"\n",
        "  prompt = \"\"\n",
        "  for idx, message in enumerate(conv):\n",
        "    to_append = f\"{'AI' if idx%2 else 'Human'}: {message}\\n\"\n",
        "    prompt += to_append\n",
        "  return prompt.strip()\n",
        "\n",
        "\n",
        "cur_conv = conversation\n",
        "# Set up a conversational loop.\n",
        "while (1):\n",
        "  print(form_prompt(cur_conv))\n",
        "  print(\"\\n[To end chatting, type 'q' then press enter.]\")\n",
        "  user_message = input(\"Human: \")\n",
        "  if user_message.strip() == \"q\":\n",
        "    break\n",
        "  cur_conv.append(user_message)\n",
        "  clear_output()\n",
        "  prompt = form_prompt(cur_conv + [\"\"])\n",
        "  response = engine.generate(prompt, num_tokens=64)\n",
        "  response = response.strip().split(\"\\n\")[0]\n",
        "  cur_conv.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HX6m3cRk_5gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUROSKw9jQvy"
      },
      "source": [
        "# Unit 2: Comparing Different Models\n",
        "\n",
        "In Unit 1, you learned how to generate text given a prompt and how to analyze model likelihoods.\n",
        "\n",
        "In this unit, we will look at how different models behave on the same prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8YeW-3Ko_Da"
      },
      "source": [
        "## 2.1 With and Without Instruction Tuning\n",
        "\n",
        "Let's take a look at the differences between pre-trained models trained mostly on internet data and finetuned models which have trained to exhibit alignment with user (or model creator) goals.\n",
        "\n",
        "You can either choose to complete this module using OpenAI's models or open-source models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofpUTaDq9bX"
      },
      "source": [
        "### [PICK ONE] GPT-3 v1 vs GPT 3.5\n",
        "The original GPT-3 was trained on a mixture of internet, book, and other data.\n",
        "It did not have any alignment-based finetuning.\n",
        "GPT 3.5 is an updated version of GPT-3 that was optmized to expect a chat format, perform instruction-following, and have safer responses.\n",
        "\n",
        "*Note: You'll need to run the OpenAI GPT-3 code block in Unit 1 for the following code block to run.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y79Yz1p5q5SN"
      },
      "outputs": [],
      "source": [
        "pretrained_engine = OpenAIEngine(\"davinci\")\n",
        "aligned_engine = OpenAIEngine(\"text-davinci-003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jDpHXVprBKY"
      },
      "source": [
        "### [PICK ONE] LLaMA vs. Alpaca\n",
        "\n",
        "As mentioned in Unit 1, LLaMA is a model pre-trained mostly on webtext.\n",
        "[**Alpaca**](https://crfm.stanford.edu/2023/03/13/alpaca.html) is a finetuned version of LLaMa that was built by researchers from Stanford.\n",
        "It is designed for academic research and to promote accessibility to models with capabilities similar to closed-source models like OpenAI's text-davinci-003. Based on Meta's LLaMA 7B model, Alpaca is trained on instruction-following demonstrations in the same style as the ones used for text-davinci-003. It has demonstrated similar behaviors to OpenAI's text-davinci-003 in various aspects while being smaller and more easily reproducible.\n",
        "\n",
        "*Note: You'll need to run the LLaMa code block in Unit 1 for the following code block to run.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3UuQpWzrHEK"
      },
      "outputs": [],
      "source": [
        "pretrained_engine = LlamaEngine(\"LLaMA\")\n",
        "aligned_engine = LlamaEngine(\"Alpaca\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eSwSsPV8PmT"
      },
      "source": [
        "## ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Can you identify prompts where the behavior of the two modelsa is very different? Some suggestions are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HTsZ1EG8fxi",
        "outputId": "44544550-4e9c-4439-b1d0-39c30af5a509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRE-TRAINED MODEL:\n",
            "”  “A hippopotamus?”  “Yes.”  “A fairy?”  “Yes.”  “A hippopotamus\n",
            "encountering a fairy?”  “Yes.”  “\n",
            "\n",
            "ALIGNED MODEL:\n",
            "  Once upon a time, there was a large and friendly hippopotamus who\n",
            "lived in a river in the middle of a lush and beautiful forest. One\n",
            "day, while the hippo was taking a leisurely swim, he noticed a tiny\n",
            "sparkle in the corner of his eye. He looked closer and saw a\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Tell me a story about a hippopotamus encountering a fairy.\"\n",
        "\n",
        "print(\"PRE-TRAINED MODEL:\")\n",
        "print(textwrap.fill(pretrained_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nALIGNED MODEL:\")\n",
        "print(textwrap.fill(aligned_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIPUsx30-lJP",
        "outputId": "6851f7f7-08dd-46a1-b49e-4d2905d23ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRE-TRAINED MODEL:\n",
            " I have a B.S. in Biology and a minor in Chemistry. I have a 3.8 GPA\n",
            "and a 161 on the GRE. I have been accepted to the University of\n",
            "Washington, University of California, Davis, and the University of\n",
            "California, San Diego. I am leaning towards UC Davis, but I am\n",
            "\n",
            "ALIGNED MODEL:\n",
            "  The best advice is to research the different graduate programs\n",
            "available and find the one that best fits your academic and career\n",
            "goals. Consider factors such as location, cost, program length,\n",
            "faculty, and curriculum. Talk to current students and alumni of the\n",
            "programs you are considering to get a better understanding of the\n",
            "school and its\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I need advice on where to go to grad school.\"\n",
        "\n",
        "print(\"PRE-TRAINED MODEL:\")\n",
        "print(textwrap.fill(pretrained_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nALIGNED MODEL:\")\n",
        "print(textwrap.fill(aligned_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Comparing model sizes\n",
        "\n",
        "Lets compare four different model sizes to see how good their generations are on a few different prompts. We'll use OpenAI's API for this. Unfortunately, OpenAI doesn't tell us the actual sizes of their GPT-3 models, but they do tell us the relative sizes."
      ],
      "metadata": {
        "id": "Himg-hkJgi5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The smallest model size released for GPT-3.\n",
        "gpt3_ada_engine = OpenAIEngine(\"ada\")\n",
        "# The medium-sized model released for GPT-3.\n",
        "gpt3_curie_engine = OpenAIEngine(\"curie\")\n",
        "# The largest GPT-3 released.\n",
        "gpt3_davinci_engine = OpenAIEngine(\"davinci\")"
      ],
      "metadata": {
        "id": "COR2pKd8gs9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does model size impact factuality?\n",
        "As model size gets larger, models tend to get better at giving correct facts. You can try fact-checking the generations from each of the models below."
      ],
      "metadata": {
        "id": "-wdMjUFIhSk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Monongahela River is located\"\n",
        "\n",
        "print(\"Small-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_ada_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nMedium-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_curie_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nFull-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_davinci_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXQn1lCThW3P",
        "outputId": "9e86c3bd-99a8-4feb-dfc8-93a651560b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small-sized GPT-3:\n",
            " in the U.S. state of Pennsylvania. The Monongahela River flows\n",
            "through the communities of Monongahela, Monongahela, and Monongahela\n",
            "Falls. The Monongahela River is a tributary of the Ohio River. The\n",
            "Monongahela River is a major t\n",
            "\n",
            "Medium-sized GPT-3:\n",
            " in the U.S. state of Pennsylvania. It is a tributary of the Allegheny\n",
            "River, which it joins at the city of Pittsburgh. The Monongahela River\n",
            "is approximately long.  The Monongahela River rises in the\n",
            "southwestern corner of Washington County, Pennsylvania, in the\n",
            "Allegheny\n",
            "\n",
            "Full-sized GPT-3:\n",
            " in the southwestern part of Pennsylvania. It is a tributary of the\n",
            "Ohio River, which is part of the Mississippi River watershed. The\n",
            "river is approximately long and is formed by the confluence of the\n",
            "West Fork River and the Little Monongahela River. The river's\n",
            "watershed is located in parts of ten counties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Language Technologies Institute at Carnegie Mellon University was founded\"\n",
        "\n",
        "print(\"Small-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_ada_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nMedium-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_curie_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nFull-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_davinci_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLt2dUspiCAm",
        "outputId": "4b4814f0-a598-4caf-dda1-9f28d75d4879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small-sized GPT-3:\n",
            " in 2010 to advance the field of language technologies. The Institute\n",
            "is a consortium of universities and research institutes that are\n",
            "working together to develop and apply new technologies to the study of\n",
            "language. The Institute is also a member of the Association for\n",
            "Computational Linguistics (ACL), the Association for Computational\n",
            "Linguistics\n",
            "\n",
            "Medium-sized GPT-3:\n",
            " in 1988 to support research in the areas of natural language\n",
            "processing, machine translation, and computational linguistics. The\n",
            "institute is located in the School of Computer Science at Carnegie\n",
            "Mellon University in Pittsburgh, PA.  The institute is home to the CMU\n",
            "NLP Group, which is a research group within the School of Computer\n",
            "\n",
            "Full-sized GPT-3:\n",
            " in 1986 by Raj Reddy. The institute is a research center for the\n",
            "study of language technologies, including speech recognition, machine\n",
            "translation, information extraction, and computational linguistics.\n",
            "The institute is also home to the National Center for Text Mining.\n",
            "The institute is located in the Gates and Hillman centers on the\n",
            "Carnegie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does model size impact coherence?\n",
        "\n",
        "Even the \"small\" versions of GPT-3 tend to be able to generate coherent text samples that use correct grammar and sound like plausible English sentences. However, the generations from smaller models tend to be more repetative than the generations from larger ones.\n",
        "\n",
        "To combat this, for smaller models, you may need to choose a bigger value of $p$ than for larger models. You can try experimenting with modifying the $p$ value below."
      ],
      "metadata": {
        "id": "k8egum-MiZW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time, there was a dragon.\"\n",
        "\n",
        "print(\"Small-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_ada_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nMedium-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_curie_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))\n",
        "\n",
        "print(\"\\nFull-sized GPT-3:\")\n",
        "print(textwrap.fill(gpt3_davinci_engine.generate(prompt, num_tokens=64, top_p=0.1, num_samples=1)))"
      ],
      "metadata": {
        "id": "hZ4zAnfKi1Fy",
        "outputId": "e9901d22-323b-40c9-d279-b2f5981754d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small-sized GPT-3:\n",
            "  It was a dragon that lived in the mountains of the south. It was a\n",
            "dragon that was a little bit of everything. It was a dragon that was a\n",
            "little bit of everything, but it was also a dragon that was a little\n",
            "bit of everything. It was a dragon that was a little bit of everything\n",
            "\n",
            "Medium-sized GPT-3:\n",
            " He was a very old dragon, and he had a very old wife. They lived in a\n",
            "very old castle, and they had a very old son. The son was very old,\n",
            "too, and he had a very old dog. The dog was very old, and he had a\n",
            "very old bone. The bone was\n",
            "\n",
            "Full-sized GPT-3:\n",
            " He was a very nice dragon, and he lived in a cave. He had a lot of\n",
            "gold, and he was very happy.  One day, a knight came to the cave. The\n",
            "knight was very brave, and he wanted to fight the dragon.  “I will\n",
            "fight you,”\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c7947f1ff06495aac7f77a9e5cb0db8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23470d5bfb7840db97516f9567523d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "398e8d5f201b4133b379169a30289fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d35c2cb34747599baeb80a5ba8394c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7808bb08fa4f4da7b6b3824dd7a28e99",
            "placeholder": "​",
            "style": "IPY_MODEL_5a007e4a0fce424f911110c8da07f1c2",
            "value": " 2/2 [00:00&lt;00:00, 59.80it/s]"
          }
        },
        "5a007e4a0fce424f911110c8da07f1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7808bb08fa4f4da7b6b3824dd7a28e99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "910d6c0192e8411e8ebc651b81a41789": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c28e3f8329c4c59a0bfc74e2beb42a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_910d6c0192e8411e8ebc651b81a41789",
            "placeholder": "​",
            "style": "IPY_MODEL_c20957e27af84bed93f00cd70f35d60a",
            "value": "100%"
          }
        },
        "c20957e27af84bed93f00cd70f35d60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc633265888d4a71911719f259e74db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7947f1ff06495aac7f77a9e5cb0db8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23470d5bfb7840db97516f9567523d64",
            "value": 2
          }
        },
        "f818c0591c0f4008825f27b8854eef4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c28e3f8329c4c59a0bfc74e2beb42a7",
              "IPY_MODEL_dc633265888d4a71911719f259e74db6",
              "IPY_MODEL_47d35c2cb34747599baeb80a5ba8394c"
            ],
            "layout": "IPY_MODEL_398e8d5f201b4133b379169a30289fdd"
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}